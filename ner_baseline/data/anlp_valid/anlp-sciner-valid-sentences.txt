Our experiments on two major triple-to-text datasets-WebNLG and E2E-show that our approach enables D2T generation from RDF triples in zero-shot settings. 
Compared with previous methods, our method achieves state-of-the-art performance with a F1 improvement from 40.2 to 45.4 on test set. 
Anther factor that makes the out-of-coverage PPL smaller when K = 500 is that there are more (simpler) examples in the set compared to K > 500, and the relatively simple utterances will also bring down the PPL. 
The data for training is the same for both XGPT-C and XPyMT5, and consists of all 5+ star GitHub repositories which are primarily Python, filtered by files which were either Python 3 compliant or were successfully fixed by lib2to3. 
In future work, we want to further test our proposed typos-aware training with more real-world typo queries by acquiring a real query log with typos and perform relevance annotations on the MS MARCO passage collection. 
For the audio modality, we use mel-spectrograms with a window size of 25 ms and stride of 12.5 ms and then chunk the spectrograms per 400 ms time window. 
We thus explore and propose alternative evaluation measures: the reported humanevaluation analysis shows that the proposed metrics, based on Question Answering, favorably compares to ROUGE -with the additional property of not requiring reference summaries. 
Text style transfer is an important Natural Language Generation (NLG) task, and has wideranging applications from adapting conversational style in dialogue agents (Zhou et al, 2017), obfuscating personal attributes (such as gender) to prevent privacy intrusion (Reddy and Knight, 2016), altering texts to be more formal or informal (Rao and Tetreault, 2018), to generating poetry . 
The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC dataset, improving the best performance (accuracy) by 2∼4%. 
We extend these rules for the cross-lingual AMR parsing task based on several multilingual resources such as Wikipedia, BabelNet 4.0 (Navigli and Ponzetto, 2010), DBpedia Spotlight API (Daiber et al, 2013) cation in all languages but Chinese, for which we use Babelfy (Moro et al, 2014) instead, Stanford CoreNLP for English preprocessing pipeline, the Stanza Toolkit (Qi et al, 2020) for Chinese, German and Spanish sentences, and Tint 3 (Aprosio and Moretti, 2016) for Italian. 
For the negative training samples collection, we randomly select generated headlines from a pointer generator (See et al, 2017) model trained on LCSTS dataset (Hu et al, 2015) and create a balanced training corpus which includes 351,508 training samples and 9,022 validation samples. 
The training process is iterated upon 200 epochs and early stopping (Yuan et al, 2007) is applied when the validation loss stops decreasing by 10 epochs. 
They are partially observable Markov decision processes (POMDPs), represented as a 7-tuple of S, T, A, Ω, O, R, γ representing the set of environment states, conditional transition probabilities between states, the vocabulary or words used to compose action commands or dialogue utterances (e.g. get sword or Hey, give me that sword! respectively), observations returned by the game, observation conditional probabilities, reward function, and the discount factor respectively. 
It is trained on the CLUECor-pusSmall dataset of 14GB , which consists of Chinese news, Wikipedia, online forum message, and consumer comments. 
In this work, we propose two additional variations of stance detection: zero-shot stance detection (a classifier is evaluated on a large number of completely new topics) and few-shot stance detection (a classifier is evaluated on a large number of topics for which it has very few training examples). 
Extensive experimental results show that, the proposed method achieves consistent improvement over the stateof-the-art baselines including kNN-MT, while maintaining the same training and decoding speed as the standard NMT model. 
Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al, 2017), text summarization (Hardy and Vlachos, 2018;Liao et al, 2018), paraphrase detection (Issa et al, 2018), spoken language understanding (Damonte et al, 2019), machine translation (Song et al, 2019b) and human-robot interaction (Bonial et al, 2020). 
In particular, PROC-B (Glavaš et al, 2019), a supervised CLWE framework that simply applies multiple iterations of 2 OPA, has been demonstrated to produce very competitive performance on various benchmark tasks including BLI as well as cross-lingual transfer for NLI and information retrieval. 
Moreover, when we remove the state transition prediction task and don't fine-tune our model with adaptive objective (only CHAN remains), the joint accuracy decreases by 1.55%. 
We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. 
In terms of performance, with a top-p value from 0.9 to 0.5, there is no significant drop in the evaluation performance. 
For all the experiments, we use pretrained 300-dimensional Glove 3 vectors (Pennington et al, 2014) to initialize the word embeddings. 
We see that TACOLM improves by 4% and 8% on the coreference task and the parent-child tasks over BERT, respectively. 
Still surprisingly, our model generalizes reasonably to compositionally novel (outof-coverage) splits, registering 30%∼50% accuracies, in contrast to HB19 reporting accuracies smaller than 10% on similar benchmarks for OVERNIGHT. 
The improvements obtained by the ∞-former are larger on the PG19 dataset, which can be justified by the nature of the datasets: books have more long range dependencies than Wikipedia articles (Rae et al, 2019). 
Parameters: For both Reptile and our method, we choose K by searching the grid {1, 3, 5, 10}, η by {0.01, 0.05, 0.1, 0.3, 0.5} for the task adaptation step, and choose τ = 1 for the meta update. 
On the IWSLT'14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU and set the maximum batch size to be 4096. 
We find that ESCHER predictions have an average Jaccard similarity with the gold predictions of 0.49, whereas the random baseline achieves 0.27. 
We focus on the text classification problem, where the dominant approach to using these nonneural models is to first calculate the number of unique terms in the dataset (the vocabulary, size V ) and encode each instance of the dataset into a bag-of-words (BoW) representation (Joachims, 1998;Zhang et al, 2010). 
Therefore, in multimodal affective computing tasks, such as emotion recognition, there are usually three modalities: textual, acoustic, and visual. 
The results show that our proposed method can converge and outperform several state-of-the-art multi-task text classification methods. 
We set the numbers of samples as N = 100 and M = 512. 
In which, each convolutional layer has 64 filters, each kernel's size is 7, there are 2 such convolutional layers that share weights. 
Instead of learning g V and g T from scratch, we build on a pre-trained CLIP model, which has been pre-trained on We-bImageText (WIT), a dataset of 400 million imagetext pairs gathered from the internet (Radford et al, 2021). 
This result shows that although both GBDA and BAE produce detectable changes, our method is slightly less perceptible than BAE but the model accuracy after attack is significantly lower for our attack: 4.7% for GBDA compared to 12.0% for BAE (cf. 
HYPMIX shows maximum improvement when applied on extremely low training data, with samples in order of n = 10. 
Compared to the KL analysis which focuses on the relative performance of the algorithms, the human evaluation highlights the realizable generation quality enabled specifically by large pretrained language models. 
We attribute the minor improvement over BM25 on TriviaQA to a high overlap between questions and passages, which gives term-based retriever a clear advantage. 
However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). 
This paper explores a novel architecture for making use of such information from knowledge bases by tying a coreference resolution system to a relation extraction system, enabling us to reward the coreference system for making predictions that lead us to infer facts that are consistent with such knowledge bases. 
In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. 
For SciERC, we use the indomain scibert-scivocab-uncased (Beltagy et al, 2019) encoder. 
In each iteration, we train the semantic parser for 30 epochs with a batch size of 64. 
Precision@1 rises from 11.5% (XLMR+SAP en syn ) to 30.9% ↑19.4% (XLMR+SAP all syn (+en-th wt+ muse)), achieved through the synergistic effect of both knowledge types: 1) UMLS synonyms in other languages push the scores to 20.6% ↑9.1% ; 2) translation knowledge increases it further to 30.9% ↑10.3% . 
MASS has achieved significant improvements in several sequence-to-sequence tasks, such as neural machine translation and text summarization (Song et al, 2019). 
We find that the original Who's Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. 
In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. 
We cluster 100 bounding boxes into 32 clusters, and 32 cluster centers are the input of the image encoder. 
We use a pre-processed version of the GAD dataset provided by BioBERT, which is split for 10-fold cross-validation. 
For NER, we observe improvements over monolingual models with 0.62% and 0.26% absolute micro F1 score improvement for French and German, respectively. 
First, we can observe that query match accuracy on test data can be improved by 6.4% at most and 1.1% at least. 
For Triframes (Ustalov et al, 2018), we use its authors' original implementation 18 , and tune the parameter k in the k-NN graph construction step for different tasks and datasets to get a reasonable number of clusters. 
Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. 
Back-translation (BT; Bojar and Tamchyna 2011;Sennrich et al 2016a;Poncelas et al 2018a) is a data augmentation method that is a key ingredient for improving translation quality of neural machine translation systems (NMT; Sutskever et al 2014;Bahdanau et al 2015;Gehring et al 2017;Vaswani et al 2017). 
As an example, the number of unique program templates in D par when K = 2, 000 on SCHOLAR and GEO is 1.9K and 1.7K, resp, compared to only 125 and 187 in D nat . 
To construct the provenance graph, it is obvious that we need to (1) obtain the sources that describe the statements about the claim, i.e., S D (q); (2) infer the relationship between the sources and the statements, i.e., determine the labeled edges of the provenance graph. 
Evaluation Metric For the perplexity metric to evaluate language gaps, we fine-tune a GPT-2 language model on the paraphrased canonical data D par for 1, 500 steps (150 steps for warm-up) with a batch size of 64 and a learning rate of 1e − 5. 
In order to evaluate the quality of claims and labels, three native English speakers were given 300 random samples from FAVIQ, and were asked to: (1) verify whether the claim is as natural as a human-written claim, with three possible ratings (perfect, minor issues but comprehensible, incomprehensible), and (2) predict the label of the claim (support or refute). 
Prior work has already used computational methods to predict symptom severity (Howes et al, 2014), measure counseling quality (Pérez-Rosas et al, 2018, and used topic models to support counselors during conversations (Dinakar et al, 2015). 
We tuned this parameter on the dev set of the Arguments dataset, and selected the threshold that maximized the F1, using the BM+TH selection policy. 
The encoder is a linear map to R 512 with ReLU activations, and the decoder is a linear map back to R 90,000 space with pointwise squaring. 
Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. 
However, such approaches still require external ZP prediction models, which have a low accuracy of 66%. 
As shown in Figure 2, META is an iterative framework, generating pseudo labels and training the text classifier alternatively, similar to many other weakly supervised text classification methods (Kuipers et al, 2006;Tao et al, 2015;. One iteration in META consists of the following steps: • Generate pseudo labels based on the seeds; • Train a text classifier based on pseudo labels; • Rank and select words and motif instances to expand the seeds. 
The zero-shot performance is barely better than random guessing, indicating that the model trained on FEVER is not able to generalize to our more challenging data. 
For different candidate database size (from one million to ten million), we compare the Coverage@500 metric of BM25-QS, BE-QS, and CFC-QS on the MC test set of Reddit (Figure 3). 
We propose specifying instead a parameter β skip which defines the skip cost in terms of the distribution of 1-1 alignment costs at alignment time: c skip = CDF −1 (β skip ). 
The relatively low SuccF1 is due to that in LABES-S2S, we do not apply additional dialog act modeling and reinforcement fine-tuning to encourage slot token generation as in other E2E models. 
Our pre-training procedure is efficient and outperforms BERT-based models with at least 0.09, 0.16, 0.35 absolute increase in F1 (exact match) for the three datasets. 
Among the 57.96% cases that do not exactly match ground truth utterances, only 6.3% are not complete, which still contains unresolved el-lipsis or co-reference, while 93.7% of these cases are complete with GECOR-generated words that do not match ground truth words. 
We build SentiBERT on the HuggingFace library 1 and initialize the model parameters using pre-trained BERT-base and RoBERTa-base models whose maximum length is 128, layer number is 12, and embedding dimension is 768. 
We tuned confidence thresholds on WMT18 Metrics task data using a grid of 16 log-probability points in [−3, 0], which yielded optimal thresholds (−1, −0.6). 
We set the margin m as 0.2 for fast model and 0.6 for base and inflated models. 
The other standard baseline for domain adaptation (PoolDomain) obtains a similar performance (−2.19 compared to our method) to the in-domain approach, which shows the benefits of multidomain adaptation. 
The embedding size d is set to 100 and the number of negative samples is fixed to 50. 
The task of Difficulty-Controllable Question Generation (DCQG) aims at generating questions with required difficulty levels and has recently attracted researchers' attention due to its wide application, such as facilitating certain curriculum-learningbased methods for QA systems (Sachan and Xing, 2016) and designing exams of various difficulty levels for educational purpose (Kurdi et al, 2020). 
We present results for masking BERT, RoBERTa, and DistilBERT in part-of-speech tagging, named-entity recognition, sequence classification, and reading comprehension. 
We intentionally sabotage low-capacity LSTM models by only training them using 25% of the seed data to generate synthetic responses. 
KBP (Ling and Weld, 2012) uses Wikipedia articles annotated with Freebase entries as the training set, and employs manually-annotated sentences from 2013 KBP slot filling assessment results (Ellis et al, 2012) as the extra test set. 
In this paper we take a deeper look at the efficacy of strong few-shot classification models in the more common relation extraction setting, and show that typical few-shot evaluation metrics obscure a wide variability in performance across relations. 
EReader consistently achieves higher performance using the knowledge retrieved from complete corpus, where the biggest gain of 7.86% is achieved when using five knowledge. 
Key information extraction from form-like documents is one of the fundamental tasks of document understanding that has many real-world applications. 
We initialize our context-aware ST with the sentence-level Baseline, i.e. ST+AFS, and then finetune the model for 20K steps based on the concatenation method with a batch size of around 40K subwords. 
We attribute LCAMs superior performance to (1) Using a domain-specific (biomedical) language representation model (BioBERT) at its encoding layer, (2) Applying label-specific attention prior to classifying a token as well as before classifying the mean pooled representation of an Example Input sentence Predicted labels Predicted labels P@1 P@2 
For IEMOCAP, our model obtained a weighted average F1 score of 65.22%, outperforming Di-alogueGCN by more than 1 point. 
Given that BERT remains one of the most effective models in varieties of NLP tasks, and especially that handcrafted features have relatively low dimensionality compared to BERT, we believe that the performance gain equivalent to 20% of the performance gain of BERT is substantial for this completely new application domain. 
More recently, it has been successfully used to reduce partial input biases in different fields of NLP, such as natural language inference (NLI) (Belinkov et al, 2019;Stacey et al, 2020) and visual question answering (VQA) (Ramakrishnan et al, 2018). 
Held-Out Performance Micro/macro F1 scores on the held-out test sets corresponding to their training data are 91.5/70.8 for B-D and 92.9/70.3 for B-F (Founta et al, 2018). 
The size of S2ORC makes it more than sufficient for pretraining large language models such as ELMO, BERT, ROBERTA, GPT2, and others, whose reported training data sizes are given in Table 9 for comparison. 
More closely related to our task is the context dependent semantic parsing task on the ATIS dataset (Zettlemoyer and Collins, 2009;Suhr et al, 2018) for mapping NL to database queries based on a prior history of NL and query pairs. 
We further trained an XLnet models with similar training hyperparameters and achieved an accuracy of 0.983. 
Since our UmlsBERT model is focused on augmenting the Masked LM task with clinical information from the UMLS Metathesaurus, we omit the description of the Next Sentence Prediction task and only describe the details of the Masked LM task herein. 
We use the publicly-provided train and test splits for the sst2 and 20news datasets and further derive a validation split consisting of 20% (v = 0.2) of the train split (D t ), with uniform class distribution. 
Lastly, to better understand how well the email thread summarization models perform and investigate the correlation between automatic metrics and human judgment, we ask humans to rate the "salience" (how well the model summarizes salient points) and "faithfulness" (how well the model stays true to the email thread) of model-generated summaries, as well as to perform a pairwise comparison between our best and base models. 
Baselines. (1) Ebner's (Ebner et al, 2020) is a semantic role labeling-based method with greedy decoding. (2) Zhang's (Zhang et al, 2020b) is a two-step head-based model that first predicts headwords of an argument and then expands to the full span. 
As seen, the BLEU score of best "External ZP prediction" model dramatically drops by -1.06 points, showing that this approach is heavily dependent on the results of external ZP annotations. 
TARA is collected via a rigorous process that involves rule-based distant supervision extraction from news-images data which results in 16k image examples. 
For instance, Reformer (Kitaev et al, 2020) is tested on the 64k-chunk en-wik8 dataset for unidirectional language modeling; Performer (Choromanski et al, 2021) reports masked language modeling (MLM) perplexity on the PG-19 book corpus and protein sequences; Linformer reports MLP perplexity with various input length, while most of the documents in their pretrain corpus are short documents. 
Document summarization is a task of creating a concise summary from a given document while keeping the original content. 
In Case 3, even if PinyinGPT-Concat ranks the ground truth as the second best, the top 1 prediction still makes much sense and fit well with the context. 
The learning rate is 5e-5 and the training steps are 5000 for both base and large models. 
For the second approach, instead, i.e., GOLDAMR-SILVERTRNS, we choose AMR 2.0 as gold dataset and translate the sentences into Chinese, German, Italian and Spanish. 
On OpenSQuAD dataset, our DUREPA model using hybrid evidences achieves a new state-ofthe-art EM score of 57.0. 
In multilingual settings, we tested Babylon multilingual word embeddings and MUSE (Lample et al, 2017) on the different tasks. 
XQuAD (Artetxe et al, 2020) is a human translation of the SQuAD en development set in 10 languages (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese), providing 1k QA pairs for each language. 
Figure 3 shows the confusion matrices of PDTB 3.0 L2 classification predictions, again from XLNet-large and BERT-large models (we did not observe immediate qualitative differences between XLNet and BERT, or between large and base models). 
That is, EPT-X generates different solution equations when it only receives the generated explanation as input in Phase 2. 
To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. 
As shown in Figure 3, the performance gap be- tween our approach and the baseline goes widest for AMR graphs with more than 5 reentrancies, on which our approach outperforms the baseline by 6.61 BLEU scores. 
We also evaluate the transfer performance of multilingual sentence embeddings on downstream classification tasks from the SentEval benchmark (Conneau and Kiela, 2018). 
Regardless of the initialization method, the VECO initialized model can gain consistent 1∼2 BLEU improvement over the randomly initialized model. 
To alleviate the issue, we train a binary classifier on the CoLA corpus (Warstadt et al, 2019) to learn to judge the grammaticality, and then filter out those examples that are classified as ungrammatical (the classifier score less than 0.5). 
In our experiments, we keep K = 96 (out of 12 × 12 = 144) heads in GPT-2 and we report the average of 5 runs with different random seeds. 
We empirically compare a wider range of model depths for Transformer+DS-Init+MAtt with up to 30 layers. 
Importantly, we create pseudo-pretrained embeddings for these new OA-NER-based tokens by adding a small amount of random Gaussian noise (mean 0 and variance of 0.1) to pre-trained embeddings (Pennington et al, 2014) of the root word corresponding to the category (e.g., person). 
We use the Adam optimizer with a learning rate of 2e-5 with warm-up. 
Then, we apply SummaReranker on XSum and Reddit TIFU, as shown in Table 6. 
In this paper, we train grammatical role probes on the embedding spaces of BERT and GPT-2 1 , and evaluate them on these rare non-prototypical examples, where the meaning of words in context is different from what we would expect from looking at the words alone. 
Paraphraser We finetune the paraphraser using a batch size of 1, 024 tokens for 5, 000 iterations (500 for warm-up), with a learning rate of 3e − 5 using ADAM. 
Models from Related Tasks EM-DM achieves very high precision (P , P w ) for the minority classes, showing a clear link between the tasks of emotion recognition and detecting changes in a user's mood -indeed, emotionally informed mod-els have been successfully applied to post-level classification tasks in mental health (Sawhney et al, 2020a); however, both EM models achieve low recall (R, R w ) for IE/IS compared to the rest. 
Experiments on the publicly available data sets, i.e., WMT-5 and OPUS-100, show that the proposed method achieves substantial improvements over strong baselines. 
We found that the annotators disagree substantially more as compared to the humanwritten answers, with a Fleiss kappa of 0.31 (vs. 0.45 for human-written answers), suggesting that the discourse structure of model-generated answers are less clear, even to our trained annotators. 
