Our	O
experiments	O
on	O
two	O
major	O
triple-to-text	B-TaskName
datasets-WebNLG	O
and	O
E2E-show	B-DatasetName
that	O
our	O
approach	O
enables	O
D2T	B-TaskName
generation	I-TaskName
from	O
RDF	O
triples	O
in	O
zero-shot	O
settings.	O

Compared	O
with	O
previous	O
methods,	O
our	O
method	O
achieves	O
state-of-the-art	O
performance	O
with	O
a	O
F1	B-MetricName
improvement	O
from	O
40.2	B-MetricValue
to	O
45.4	B-MetricValue
on	O
test	O
set.	O

Anther	O
factor	O
that	O
makes	O
the	O
out-of-coverage	O
PPL	O
smaller	O
when	O
K	B-HyperparameterName
=	O
500	B-HyperparameterValue
is	O
that	O
there	O
are	O
more	O
(simpler)	O
examples	O
in	O
the	O
set	O
compared	O
to	O
K	B-HyperparameterName
>	O
500,	B-HyperparameterValue
and	O
the	O
relatively	O
simple	O
utterances	O
will	O
also	O
bring	O
down	O
the	O
PPL.	O

The	O
data	O
for	O
training	O
is	O
the	O
same	O
for	O
both	O
XGPT-C	B-DatasetName
and	O
XPyMT5,	B-DatasetName
and	O
consists	O
of	O
all	O
5+	O
star	O
GitHub	O
repositories	O
which	O
are	O
primarily	O
Python,	O
filtered	O
by	O
files	O
which	O
were	O
either	O
Python	O
3	O
compliant	O
or	O
were	O
successfully	O
fixed	O
by	O
lib2to3.	O

In	O
future	O
work,	O
we	O
want	O
to	O
further	O
test	O
our	O
proposed	O
typos-aware	B-TaskName
training	O
with	O
more	O
real-world	O
typo	O
queries	O
by	O
acquiring	O
a	O
real	O
query	O
log	O
with	O
typos	O
and	O
perform	O
relevance	O
annotations	O
on	O
the	B-DatasetName
MS	I-DatasetName
MARCO	I-DatasetName
passage	O
collection.	O

For	O
the	O
audio	O
modality,	O
we	O
use	O
mel-spectrograms	B-MethodName
with	O
a	O
window	B-HyperparameterName
size	I-HyperparameterName
of	O
25	B-HyperparameterValue
ms	I-HyperparameterValue
and	O
stride	B-HyperparameterName
of	O
12.5	B-HyperparameterValue
ms	I-HyperparameterValue
and	O
then	O
chunk	O
the	O
spectrograms	O
per	O
400	B-HyperparameterValue
ms	O
time	O
window.	O

We	O
thus	O
explore	O
and	O
propose	O
alternative	O
evaluation	O
measures:	O
the	O
reported	O
humanevaluation	B-MetricName
analysis	O
shows	O
that	O
the	O
proposed	O
metrics,	O
based	O
on	B-TaskName
Question	I-TaskName
Answering,	I-TaskName
favorably	O
compares	O
to	O
ROUGE	B-MetricName
-with	O
the	O
additional	O
property	O
of	O
not	O
requiring	O
reference	O
summaries.	O

Text	B-TaskName
style	I-TaskName
transfer	I-TaskName
is	O
an	O
important	B-TaskName
Natural	I-TaskName
Language	I-TaskName
Generation	I-TaskName
(NLG)	B-TaskName
task,	O
and	O
has	O
wideranging	O
applications	O
from	O
adapting	O
conversational	O
style	O
in	O
dialogue	O
agents	O
(Zhou	O
et	O
al,	O
2017),	O
obfuscating	O
personal	O
attributes	O
(such	O
as	O
gender)	O
to	O
prevent	O
privacy	O
intrusion	O
(Reddy	O
and	O
Knight,	O
2016),	O
altering	O
texts	O
to	O
be	O
more	O
formal	O
or	O
informal	O
(Rao	O
and	O
Tetreault,	O
2018),	O
to	O
generating	O
poetry	O
.	O

The	O
proposed	O
model	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
the	O
miniRCV1	B-DatasetName
and	B-DatasetName
ODIC	I-DatasetName
dataset,	O
improving	O
the	O
best	O
performance	O
(accuracy)	B-MetricName
by	O
2∼4%.	B-MetricValue

We	O
extend	O
these	O
rules	O
for	O
the	O
cross-lingual	B-TaskName
AMR	I-TaskName
parsing	I-TaskName
task	O
based	O
on	O
several	O
multilingual	O
resources	O
such	O
as	O
Wikipedia,	B-DatasetName
BabelNet	B-DatasetName
4.0	I-DatasetName
(Navigli	O
and	O
Ponzetto,	O
2010),	O
DBpedia	B-DatasetName
Spotlight	I-DatasetName
API	I-DatasetName
(Daiber	O
et	O
al,	O
2013)	O
cation	O
in	O
all	O
languages	O
but	O
Chinese,	O
for	O
which	O
we	O
use	O
Babelfy	B-DatasetName
(Moro	O
et	O
al,	O
2014)	O
instead,	O
Stanford	B-DatasetName
CoreNLP	I-DatasetName
for	O
English	O
preprocessing	O
pipeline,	O
the	O
Stanza	B-DatasetName
Toolkit	O
(Qi	O
et	O
al,	O
2020)	O
for	O
Chinese,	O
German	O
and	O
Spanish	O
sentences,	O
and	O
Tint	B-DatasetName
3	I-DatasetName
(Aprosio	O
and	O
Moretti,	O
2016)	O
for	O
Italian.	O

For	O
the	O
negative	O
training	O
samples	O
collection,	O
we	O
randomly	O
select	O
generated	O
headlines	O
from	O
a	O
pointer	O
generator	O
(See	O
et	O
al,	O
2017)	O
model	O
trained	O
on	O
LCSTS	I-DatasetName
dataset	O
(Hu	O
et	O
al,	O
2015)	O
and	O
create	O
a	O
balanced	O
training	O
corpus	O
which	O
includes	O
351,508	O
training	O
samples	O
and	O
9,022	O
validation	O
samples.	O

The	O
training	O
process	O
is	O
iterated	O
upon	O
200	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
early	O
stopping	O
(Yuan	O
et	O
al,	O
2007)	O
is	O
applied	O
when	O
the	O
validation	O
loss	O
stops	O
decreasing	O
by	O
10	B-HyperparameterValue
epochs.	B-HyperparameterName

They	O
are	O
partially	O
observable	O
Markov	B-MethodName
decision	I-MethodName
processes	I-MethodName
(POMDPs),	B-MethodName
represented	O
as	O
a	O
7-tuple	O
of	O
S,	O
T,	O
A,	O
Ω,	O
O,	O
R,	O
γ	O
representing	O
the	O
set	O
of	O
environment	O
states,	O
conditional	O
transition	O
probabilities	O
between	O
states,	O
the	O
vocabulary	O
or	O
words	O
used	O
to	O
compose	O
action	O
commands	O
or	O
dialogue	O
utterances	O
(e.g.	O
get	O
sword	O
or	O
Hey,	O
give	O
me	O
that	O
sword!	O
respectively),	O
observations	O
returned	O
by	O
the	O
game,	O
observation	O
conditional	O
probabilities,	O
reward	O
function,	O
and	O
the	O
discount	O
factor	O
respectively.	O

It	O
is	O
trained	O
on	O
the	B-DatasetName
CLUECor-pusSmall	I-DatasetName
dataset	O
of	O
14GB	O
,	O
which	O
consists	O
of	O
Chinese	O
news,	O
Wikipedia,	O
online	O
forum	O
message,	O
and	O
consumer	O
comments.	O

In	O
this	O
work,	O
we	O
propose	O
two	O
additional	O
variations	O
of	O
stance	B-TaskName
detection:	I-TaskName
zero-shot	B-TaskName
stance	I-TaskName
detection	I-TaskName
(a	O
classifier	O
is	O
evaluated	O
on	O
a	O
large	O
number	O
of	O
completely	O
new	O
topics)	O
and	O
few-shot	B-TaskName
stance	I-TaskName
detection	I-TaskName
(a	O
classifier	O
is	O
evaluated	O
on	O
a	O
large	O
number	O
of	O
topics	O
for	O
which	O
it	O
has	O
very	O
few	O
training	O
examples).	O

Extensive	O
experimental	O
results	O
show	O
that,	O
the	O
proposed	O
method	O
achieves	O
consistent	O
improvement	O
over	O
the	O
stateof-the-art	O
baselines	O
including	O
kNN-MT,	B-MethodName
while	O
maintaining	O
the	O
same	O
training	O
and	O
decoding	O
speed	O
as	O
the	O
standard	O
NMT	B-MethodName
model.	O

Such	O
representations	O
are	O
actively	O
integrated	O
in	O
several	B-TaskName
Natural	I-TaskName
Language	I-TaskName
Processing	I-TaskName
(NLP)	B-TaskName
applications,	O
inter	O
alia,	O
information	I-TaskName
extraction	I-TaskName
(Rao	O
et	O
al,	O
2017),	O
text	I-TaskName
summarization	I-TaskName
(Hardy	O
and	O
Vlachos,	O
2018;Liao	O
et	O
al,	O
2018),	O
paraphrase	B-TaskName
detection	I-TaskName
(Issa	O
et	O
al,	O
2018),	O
spoken	B-TaskName
language	I-TaskName
understanding	I-TaskName
(Damonte	O
et	O
al,	O
2019),	O
machine	I-TaskName
translation	I-TaskName
(Song	O
et	O
al,	O
2019b)	O
and	O
human-robot	B-TaskName
interaction	I-TaskName
(Bonial	O
et	O
al,	O
2020).	O

In	O
particular,	O
PROC-B	B-DatasetName
(Glavaš	O
et	O
al,	O
2019),	O
a	O
supervised	O
CLWE	B-MethodName
framework	O
that	O
simply	O
applies	O
multiple	O
iterations	O
of	O
2	O
OPA,	O
has	O
been	O
demonstrated	O
to	O
produce	O
very	O
competitive	O
performance	O
on	O
various	O
benchmark	O
tasks	O
including	O
BLI	B-TaskName
as	O
well	O
as	O
cross-lingual	B-TaskName
transfer	I-TaskName
for	O
NLI	B-TaskName
and	B-TaskName
information	I-TaskName
retrieval.	I-TaskName

Moreover,	O
when	O
we	O
remove	O
the	O
state	O
transition	O
prediction	O
task	O
and	O
don't	O
fine-tune	O
our	O
model	O
with	O
adaptive	O
objective	O
(only	O
CHAN	O
remains),	O
the	O
joint	O
accuracy	B-MetricName
decreases	O
by	O
1.55%.	B-MetricValue

We	O
also	O
find,	O
however,	O
that	O
performance	O
is	O
heavily	O
influenced	O
by	O
word	O
frequency,	O
with	O
experiments	O
showing	O
that	O
both	O
the	O
absolute	O
frequency	O
of	O
a	O
verb	O
form,	O
as	O
well	O
as	O
the	O
frequency	O
relative	O
to	O
the	O
alternate	O
inflection,	O
are	O
causally	O
implicated	O
in	O
the	O
predictions	O
BERT	B-MethodName
makes	O
at	O
inference	O
time.	O

In	O
terms	O
of	O
performance,	O
with	O
a	O
top-p	B-HyperparameterName
value	O
from	O
0.9	B-HyperparameterValue
to	O
0.5,	B-HyperparameterValue
there	O
is	O
no	O
significant	O
drop	O
in	O
the	O
evaluation	O
performance.	O

For	O
all	O
the	O
experiments,	O
we	O
use	O
pretrained	O
300-dimensional	B-HyperparameterValue
Glove	B-MethodName
3	O
vectors	O
(Pennington	O
et	O
al,	O
2014)	O
to	O
initialize	O
the	O
word	O
embeddings.	O

We	O
see	O
that	O
TACOLM	B-MethodName
improves	O
by	O
4%	B-MetricValue
and	O
8%	B-MetricValue
on	O
the	B-TaskName
coreference	I-TaskName
task	O
and	O
the	O
parent-child	B-TaskName
tasks	O
over	O
BERT,	B-MethodName
respectively.	O

Still	O
surprisingly,	O
our	O
model	O
generalizes	O
reasonably	O
to	O
compositionally	O
novel	O
(outof-coverage)	O
splits,	O
registering	O
30%∼50%	B-MetricValue
accuracies,	B-MetricName
in	O
contrast	O
to	O
HB19	B-MethodName
reporting	O
accuracies	B-MetricName
smaller	O
than	O
10%	B-MetricValue
on	O
similar	O
benchmarks	O
for	O
OVERNIGHT.	B-DatasetName

The	O
improvements	O
obtained	O
by	O
the	O
∞-former	O
are	O
larger	O
on	O
the	O
PG19	B-DatasetName
dataset,	O
which	O
can	O
be	O
justified	O
by	O
the	O
nature	O
of	O
the	O
datasets:	O
books	O
have	O
more	O
long	O
range	O
dependencies	O
than	O
Wikipedia	O
articles	O
(Rae	O
et	O
al,	O
2019).	O

Parameters:	O
For	O
both	O
Reptile	B-MethodName
and	O
our	O
method,	O
we	O
choose	O
K	B-HyperparameterName
by	O
searching	O
the	O
grid	O
{1,	B-HyperparameterValue
3,	B-HyperparameterValue
5,	B-HyperparameterValue
10},	B-HyperparameterValue
η	B-HyperparameterName
by	O
{0.01,	B-HyperparameterValue
0.05,	B-HyperparameterValue
0.1,	B-HyperparameterValue
0.3,	B-HyperparameterValue
0.5}	B-HyperparameterValue
for	O
the	O
task	O
adaptation	O
step,	O
and	O
choose	O
τ	B-HyperparameterName
=	O
1	B-HyperparameterValue
for	O
the	O
meta	O
update.	O

On	O
the	O
IWSLT'14	B-DatasetName
De-En	O
dataset,	O
we	O
conduct	O
training	O
on	O
one	O
NVIDIA	O
GeForce	O
GTX	O
1080	O
Ti	O
GPU	O
and	O
set	O
the	O
maximum	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
be	O
4096.	B-HyperparameterValue

We	O
find	O
that	O
ESCHER	B-MethodName
predictions	O
have	O
an	O
average	O
Jaccard	B-MetricName
similarity	I-MetricName
with	O
the	O
gold	O
predictions	O
of	O
0.49,	B-MetricValue
whereas	O
the	O
random	O
baseline	O
achieves	O
0.27.	B-MetricValue

We	O
focus	O
on	O
the	B-TaskName
text	I-TaskName
classification	I-TaskName
problem,	O
where	O
the	O
dominant	O
approach	O
to	O
using	O
these	O
nonneural	O
models	O
is	O
to	O
first	O
calculate	O
the	O
number	O
of	O
unique	O
terms	O
in	O
the	O
dataset	O
(the	O
vocabulary,	O
size	O
V	O
)	O
and	O
encode	O
each	O
instance	O
of	O
the	O
dataset	O
into	O
a	O
bag-of-words	O
(BoW)	B-MethodName
representation	O
(Joachims,	O
1998;Zhang	O
et	O
al,	O
2010).	O

Therefore,	O
in	O
multimodal	B-TaskName
affective	I-TaskName
computing	I-TaskName
tasks,	O
such	O
as	B-TaskName
emotion	I-TaskName
recognition,	I-TaskName
there	O
are	O
usually	O
three	O
modalities:	O
textual,	O
acoustic,	O
and	O
visual.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
method	O
can	O
converge	O
and	O
outperform	O
several	O
state-of-the-art	O
multi-task	B-TaskName
text	I-TaskName
classification	I-TaskName
methods.	O

We	O
set	O
the	O
numbers	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
as	O
N	B-HyperparameterName
=	O
100	B-HyperparameterValue
and	O
M	B-HyperparameterName
=	O
512.	B-HyperparameterValue

In	O
which,	O
each	O
convolutional	O
layer	O
has	O
64	B-HyperparameterValue
filters,	B-HyperparameterName
each	O
kernel's	B-HyperparameterName
size	B-HyperparameterName
is	O
7,	B-HyperparameterValue
there	O
are	O
2	O
such	O
convolutional	O
layers	O
that	O
share	O
weights.	O

Instead	O
of	O
learning	O
g	O
V	O
and	O
g	O
T	O
from	O
scratch,	O
we	O
build	O
on	O
a	O
pre-trained	O
CLIP	B-MethodName
model,	O
which	O
has	O
been	O
pre-trained	O
on	O
We-bImageText	B-DatasetName
(WIT),	B-DatasetName
a	O
dataset	O
of	O
400	O
million	O
imagetext	O
pairs	O
gathered	O
from	O
the	O
internet	O
(Radford	O
et	O
al,	O
2021).	O

This	O
result	O
shows	O
that	O
although	O
both	O
GBDA	B-MethodName
and	O
BAE	B-MethodName
produce	O
detectable	O
changes,	O
our	O
method	O
is	O
slightly	O
less	O
perceptible	O
than	O
BAE	B-MethodName
but	O
the	O
model	B-MetricName
accuracy	I-MetricName
after	O
attack	O
is	O
significantly	O
lower	O
for	O
our	O
attack:	O
4.7%	B-MetricValue
for	O
GBDA	B-MethodName
compared	O
to	O
12.0%	B-MetricValue
for	O
BAE	B-MethodName
(cf.	O

HYPMIX	B-MethodName
shows	O
maximum	O
improvement	O
when	O
applied	O
on	O
extremely	O
low	O
training	O
data,	O
with	O
samples	O
in	O
order	O
of	O
n	B-HyperparameterName
=	O
10.	B-HyperparameterValue

Compared	O
to	O
the	O
KL	B-MetricName
analysis	O
which	O
focuses	O
on	O
the	O
relative	O
performance	O
of	O
the	O
algorithms,	O
the	O
human	B-MetricName
evaluation	I-MetricName
highlights	O
the	O
realizable	O
generation	O
quality	O
enabled	O
specifically	O
by	O
large	O
pretrained	O
language	O
models.	O

We	O
attribute	O
the	O
minor	O
improvement	O
over	O
BM25	B-MethodName
on	O
TriviaQA	B-DatasetName
to	O
a	O
high	O
overlap	O
between	O
questions	O
and	O
passages,	O
which	O
gives	O
term-based	O
retriever	O
a	O
clear	O
advantage.	O

However,	O
no	O
previous	O
work	O
has	O
addressed	O
the	O
task	O
of	O
zero-shot	B-TaskName
NERC,	I-TaskName
which	O
additionally	O
requires	O
the	O
detection	O
of	O
which	O
tokens	O
make	O
up	O
an	O
entity	O
in	O
addition	O
to	O
its	O
type,	O
i.e.	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(NER).	B-TaskName

This	O
paper	O
explores	O
a	O
novel	O
architecture	O
for	O
making	O
use	O
of	O
such	O
information	O
from	O
knowledge	O
bases	O
by	O
tying	O
a	O
coreference	B-TaskName
resolution	I-TaskName
system	O
to	O
a	O
relation	B-TaskName
extraction	I-TaskName
system,	O
enabling	O
us	O
to	O
reward	O
the	O
coreference	B-TaskName
system	O
for	O
making	O
predictions	O
that	O
lead	O
us	O
to	O
infer	O
facts	O
that	O
are	O
consistent	O
with	O
such	O
knowledge	O
bases.	O

In	O
this	O
paper,	O
we	O
provide	O
a	O
bilingual	O
parallel	B-TaskName
human-to-human	I-TaskName
recommendation	I-TaskName
dialog	O
dataset	O
(DuRecDial	B-DatasetName
2.0)	I-DatasetName
to	O
enable	O
researchers	O
to	O
explore	O
a	O
challenging	O
task	O
of	O
multilingual	B-TaskName
and	B-TaskName
cross-lingual	B-TaskName
conversational	I-TaskName
recommendation.	I-TaskName

For	B-DatasetName
SciERC,	I-DatasetName
we	O
use	O
the	O
indomain	O
scibert-scivocab-uncased	B-MethodName
(Beltagy	O
et	O
al,	O
2019)	O
encoder.	O

In	O
each	O
iteration,	O
we	O
train	O
the	O
semantic	B-TaskName
parser	I-TaskName
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64.	B-HyperparameterValue

Precision@1	B-MetricName
rises	O
from	O
11.5%	B-MetricValue
(XLMR+SAP	O
en	O
syn	O
)	O
to	O
30.9%	B-MetricValue
↑19.4%	O
(XLMR+SAP	O
all	O
syn	O
(+en-th	O
wt+	B-MethodName
muse)),	B-MethodName
achieved	O
through	O
the	O
synergistic	O
effect	O
of	O
both	O
knowledge	O
types:	O
1)	O
UMLS	B-MethodName
synonyms	O
in	O
other	O
languages	O
push	O
the	O
scores	O
to	O
20.6%	B-MetricValue
↑9.1%	O
;	O
2)	O
translation	O
knowledge	O
increases	O
it	O
further	O
to	O
30.9%	B-MetricValue
↑10.3%	O
.	O

MASS	B-MethodName
has	O
achieved	O
significant	O
improvements	O
in	O
several	O
sequence-to-sequence	O
tasks,	O
such	O
as	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
and	B-TaskName
text	I-TaskName
summarization	I-TaskName
(Song	O
et	O
al,	O
2019).	O

We	O
find	O
that	O
the	O
original	O
Who's	B-DatasetName
Waldo	I-DatasetName
dataset	O
compiled	O
for	O
this	O
task	O
contains	O
a	O
large	O
number	O
of	O
biased	O
samples	O
that	O
are	O
solvable	O
simply	O
by	O
heuristic	O
methods;	O
for	O
instance,	O
in	O
many	O
cases	O
the	O
first	O
name	O
in	O
the	O
sentence	O
corresponds	O
to	O
the	O
largest	O
bounding	O
box,	O
or	O
the	O
sequence	O
of	O
names	O
in	O
the	O
sentence	O
corresponds	O
to	O
an	O
exact	O
left-to-right	O
order	O
in	O
the	O
image.	O

In	O
this	O
paper,	O
we	O
propose	O
a	O
novel	O
model	O
of	O
Breadth	B-MethodName
First	I-MethodName
Reasoning	I-MethodName
Graph	I-MethodName
(BFR-Graph),	B-MethodName
which	O
presents	O
a	O
new	O
message	O
passing	O
way	O
that	O
better	O
conforms	O
to	O
the	O
reasoning	O
process.	O

We	O
cluster	O
100	B-HyperparameterValue
bounding	B-HyperparameterName
boxes	I-HyperparameterName
into	O
32	B-HyperparameterValue
clusters,	B-HyperparameterName
and	O
32	B-HyperparameterValue
cluster	B-HyperparameterName
centers	I-HyperparameterName
are	O
the	O
input	O
of	O
the	O
image	O
encoder.	O

We	O
use	O
a	O
pre-processed	O
version	O
of	O
the	O
GAD	B-DatasetName
dataset	O
provided	O
by	O
BioBERT,	B-DatasetName
which	O
is	O
split	O
for	O
10-fold	B-HyperparameterValue
cross-validation.	O

For	B-DatasetName
NER,	I-DatasetName
we	O
observe	O
improvements	O
over	O
monolingual	O
models	O
with	O
0.62%	B-MetricValue
and	O
0.26%	B-MetricValue
absolute	O
micro	B-MetricName
F1	I-MetricName
score	O
improvement	O
for	O
French	O
and	O
German,	O
respectively.	O

First,	O
we	O
can	O
observe	O
that	O
query	B-MetricName
match	I-MetricName
accuracy	I-MetricName
on	O
test	O
data	O
can	O
be	O
improved	O
by	O
6.4%	B-MetricValue
at	O
most	O
and	O
1.1%	B-MetricValue
at	O
least.	O

For	B-DatasetName
Triframes	B-MethodName
(Ustalov	O
et	O
al,	O
2018),	O
we	O
use	O
its	O
authors'	O
original	O
implementation	O
18	O
,	O
and	O
tune	O
the	O
parameter	O
k	B-HyperparameterName
in	O
the	O
k-NN	B-MethodName
graph	O
construction	O
step	O
for	O
different	O
tasks	O
and	O
datasets	O
to	O
get	O
a	O
reasonable	O
number	O
of	O
clusters.	O

Particularly,	O
our	O
ranker	B-MethodName
outperforms	O
the	O
conventional	O
dialog	B-MetricName
perplexity	I-MetricName
baseline	O
with	O
a	O
large	O
margin	O
on	O
predicting	O
Reddit	O
feedback.	O

Back-translation	B-TaskName
(BT;	B-MethodName
Bojar	O
and	O
Tamchyna	O
2011;Sennrich	O
et	O
al	O
2016a;Poncelas	O
et	O
al	O
2018a)	O
is	O
a	B-TaskName
data	I-TaskName
augmentation	I-TaskName
method	O
that	O
is	O
a	O
key	O
ingredient	O
for	O
improving	O
translation	O
quality	O
of	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
systems	O
(NMT;	B-TaskName
Sutskever	O
et	O
al	O
2014;Bahdanau	O
et	O
al	O
2015;Gehring	O
et	O
al	O
2017;Vaswani	O
et	O
al	O
2017).	O

As	O
an	O
example,	O
the	O
number	O
of	O
unique	O
program	O
templates	O
in	O
D	O
par	I-DatasetName
when	O
K	B-HyperparameterName
=	O
2,	B-HyperparameterValue
000	I-HyperparameterValue
on	O
SCHOLAR	B-DatasetName
and	O
GEO	B-DatasetName
is	O
1.9K	B-MetricValue
and	O
1.7K,	B-MetricValue
resp,	O
compared	O
to	O
only	O
125	B-MetricValue
and	O
187	B-MetricValue
in	O
D	B-DatasetName
nat	I-DatasetName
.	O

To	O
construct	O
the	O
provenance	O
graph,	O
it	O
is	O
obvious	O
that	O
we	O
need	O
to	O
(1)	O
obtain	O
the	O
sources	O
that	O
describe	O
the	O
statements	O
about	O
the	O
claim,	O
i.e.,	O
S	O
D	O
(q);	O
(2)	O
infer	O
the	O
relationship	O
between	O
the	O
sources	O
and	O
the	O
statements,	O
i.e.,	O
determine	O
the	O
labeled	O
edges	O
of	O
the	O
provenance	O
graph.	O

Evaluation	O
Metric	O
For	O
the	O
perplexity	B-MetricName
metric	O
to	O
evaluate	O
language	O
gaps,	O
we	O
fine-tune	O
a	O
GPT-2	B-MethodName
language	O
model	O
on	O
the	O
paraphrased	O
canonical	O
data	O
D	O
par	O
for	O
1,	B-HyperparameterValue
500	I-HyperparameterValue
steps	B-HyperparameterName
(150	B-HyperparameterValue
steps	B-HyperparameterName
for	O
warm-up)	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	B-HyperparameterValue
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	B-HyperparameterValue
−	I-HyperparameterValue
5.	I-HyperparameterValue

In	O
order	O
to	O
evaluate	O
the	O
quality	O
of	O
claims	O
and	O
labels,	O
three	O
native	O
English	O
speakers	O
were	O
given	O
300	O
random	O
samples	O
from	O
FAVIQ,	B-DatasetName
and	O
were	O
asked	O
to:	O
(1)	O
verify	O
whether	O
the	O
claim	O
is	O
as	O
natural	O
as	O
a	O
human-written	O
claim,	O
with	O
three	O
possible	O
ratings	O
(perfect,	O
minor	O
issues	O
but	O
comprehensible,	O
incomprehensible),	O
and	O
(2)	O
predict	O
the	O
label	O
of	O
the	O
claim	O
(support	O
or	O
refute).	O

Prior	O
work	O
has	O
already	O
used	O
computational	O
methods	O
to	O
predict	O
symptom	O
severity	O
(Howes	O
et	O
al,	O
2014),	O
measure	O
counseling	O
quality	O
(Pérez-Rosas	O
et	O
al,	O
2018,	O
and	O
used	O
topic	O
models	O
to	O
support	O
counselors	O
during	O
conversations	O
(Dinakar	O
et	O
al,	O
2015).	O

We	O
tuned	O
this	O
parameter	O
on	O
the	O
dev	O
set	O
of	O
the	O
Arguments	O
dataset,	O
and	O
selected	O
the	O
threshold	O
that	O
maximized	O
the	O
F1,	B-MetricName
using	O
the	O
BM+TH	B-MethodName
selection	O
policy.	O

The	O
encoder	O
is	O
a	O
linear	O
map	O
to	O
R	B-HyperparameterValue
512	B-HyperparameterValue
with	O
ReLU	O
activations,	O
and	O
the	O
decoder	O
is	O
a	O
linear	O
map	O
back	O
to	O
R	B-HyperparameterName
90,000	B-HyperparameterValue
space	O
with	O
pointwise	O
squaring.	O

Finally	O
we	O
provide	O
a	O
qualitative	O
analysis	O
which	O
sheds	O
light	O
on	O
the	O
suitability	O
of	O
AMR	B-MethodName
across	O
languages.	O

However,	O
such	O
approaches	O
still	O
require	O
external	O
ZP	O
prediction	O
models,	O
which	O
have	O
a	O
low	O
accuracy	B-MetricName
of	O
66%.	B-MetricValue

As	O
shown	O
in	O
Figure	O
2,	O
META	B-MethodName
is	O
an	O
iterative	O
framework,	O
generating	O
pseudo	O
labels	O
and	O
training	O
the	O
text	O
classifier	O
alternatively,	O
similar	O
to	O
many	O
other	O
weakly	O
supervised	O
text	O
classification	O
methods	O
(Kuipers	O
et	O
al,	O
2006;Tao	O
et	O
al,	O
2015;.	O
One	O
iteration	O
in	O
META	B-MethodName
consists	O
of	O
the	O
following	O
steps:	O
•	O
Generate	O
pseudo	O
labels	O
based	O
on	O
the	O
seeds;	O
•	O
Train	O
a	O
text	O
classifier	O
based	O
on	O
pseudo	O
labels;	O
•	O
Rank	O
and	O
select	O
words	O
and	O
motif	O
instances	O
to	O
expand	O
the	O
seeds.	O

The	O
zero-shot	O
performance	O
is	O
barely	O
better	O
than	O
random	O
guessing,	O
indicating	O
that	O
the	O
model	O
trained	O
on	O
FEVER	I-DatasetName
is	O
not	O
able	O
to	O
generalize	O
to	O
our	O
more	O
challenging	O
data.	O

For	O
different	O
candidate	O
database	O
size	O
(from	O
one	B-HyperparameterValue
million	O
to	O
ten	B-HyperparameterValue
million),	O
we	O
compare	O
the	O
Coverage@500	B-MetricName
metric	O
of	O
BM25-QS,	B-MethodName
BE-QS,	B-MethodName
and	O
CFC-QS	B-MethodName
on	O
the	O
MC	B-DatasetName
test	O
set	O
of	O
Reddit	B-DatasetName
(Figure	O
3).	O

We	O
propose	O
specifying	O
instead	O
a	O
parameter	O
β	B-HyperparameterName
skip	I-HyperparameterName
which	O
defines	O
the	O
skip	O
cost	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
1-1	O
alignment	O
costs	O
at	O
alignment	O
time:	O
c	O
skip	O
=	O
CDF	O
−1	O
(β	O
skip	O
).	O

The	O
relatively	O
low	O
SuccF1	B-MetricName
is	O
due	O
to	O
that	O
in	O
LABES-S2S,	B-DatasetName
we	O
do	O
not	O
apply	O
additional	O
dialog	O
act	O
modeling	O
and	O
reinforcement	O
fine-tuning	O
to	O
encourage	O
slot	B-TaskName
token	I-TaskName
generation	I-TaskName
as	O
in	O
other	O
E2E	B-TaskName
models.	O

Our	O
pre-training	O
procedure	O
is	O
efficient	O
and	O
outperforms	O
BERT-based	B-MethodName
models	O
with	O
at	O
least	O
0.09,	B-MetricValue
0.16,	B-MetricValue
0.35	B-MetricValue
absolute	O
increase	O
in	O
F1	B-MetricName
(exact	O
match)	I-MetricName
for	O
the	O
three	O
datasets.	O

Among	O
the	O
57.96%	B-MetricValue
cases	O
that	O
do	O
not	O
exactly	O
match	O
ground	O
truth	O
utterances,	O
only	O
6.3%	B-MetricValue
are	O
not	O
complete,	O
which	O
still	O
contains	O
unresolved	O
el-lipsis	O
or	O
co-reference,	O
while	O
93.7%	B-MetricValue
of	O
these	O
cases	O
are	O
complete	O
with	O
GECOR-generated	B-MethodName
words	O
that	O
do	O
not	O
match	O
ground	O
truth	O
words.	O

We	O
build	O
SentiBERT	B-MethodName
on	O
the	O
HuggingFace	B-DatasetName
library	O
1	O
and	O
initialize	O
the	O
model	O
parameters	O
using	O
pre-trained	O
BERT-base	B-MethodName
and	O
RoBERTa-base	B-MethodName
models	O
whose	O
maximum	B-HyperparameterName
length	I-HyperparameterName
is	O
128,	B-HyperparameterValue
layer	B-HyperparameterName
number	I-HyperparameterName
is	O
12,	B-HyperparameterValue
and	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
is	O
768.	B-HyperparameterValue

We	O
tuned	O
confidence	B-HyperparameterName
thresholds	I-HyperparameterName
on	O
WMT18	B-DatasetName
Metrics	O
task	O
data	O
using	O
a	O
grid	O
of	O
16	B-HyperparameterValue
log-probability	B-HyperparameterName
points	O
in	O
[−3,	B-HyperparameterValue
0],	B-HyperparameterValue
which	O
yielded	O
optimal	O
thresholds	O
(−1,	B-MetricValue
−0.6).	B-MetricValue

We	O
set	O
the	O
margin	B-HyperparameterName
m	B-HyperparameterName
as	O
0.2	B-HyperparameterValue
for	O
fast	O
model	O
and	O
0.6	B-HyperparameterValue
for	O
base	O
and	O
inflated	O
models.	O

The	O
other	O
standard	O
baseline	O
for	O
domain	B-TaskName
adaptation	I-TaskName
(PoolDomain)	O
obtains	O
a	O
similar	O
performance	O
(−2.19	B-MetricValue
compared	O
to	O
our	O
method)	O
to	O
the	O
in-domain	O
approach,	O
which	O
shows	O
the	O
benefits	O
of	O
multidomain	B-TaskName
adaptation.	I-TaskName

The	O
embedding	B-HyperparameterName
size	I-HyperparameterName
d	B-HyperparameterName
is	O
set	O
to	O
100	B-HyperparameterValue
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
negative	I-HyperparameterName
samples	I-HyperparameterName
is	O
fixed	O
to	O
50.	B-HyperparameterValue

The	O
task	O
of	O
Difficulty-Controllable	B-TaskName
Question	I-TaskName
Generation	I-TaskName
(DCQG)	B-TaskName
aims	O
at	O
generating	O
questions	O
with	O
required	O
difficulty	O
levels	O
and	O
has	O
recently	O
attracted	O
researchers'	O
attention	O
due	O
to	O
its	O
wide	O
application,	O
such	O
as	O
facilitating	O
certain	O
curriculum-learningbased	O
methods	O
for	O
QA	O
systems	O
(Sachan	O
and	O
Xing,	O
2016)	O
and	O
designing	O
exams	O
of	O
various	O
difficulty	O
levels	O
for	O
educational	O
purpose	O
(Kurdi	O
et	O
al,	O
2020).	O

We	O
present	O
results	O
for	O
masking	O
BERT,	B-MethodName
RoBERTa,	B-MethodName
and	O
DistilBERT	B-MethodName
in	O
part-of-speech	B-TaskName
tagging,	I-TaskName
named-entity	B-TaskName
recognition,	I-TaskName
sequence	B-TaskName
classification,	I-TaskName
and	O
reading	B-TaskName
comprehension.	I-TaskName

We	O
intentionally	O
sabotage	O
low-capacity	O
LSTM	B-MethodName
models	O
by	O
only	O
training	O
them	O
using	O
25%	B-HyperparameterValue
of	O
the	O
seed	O
data	O
to	O
generate	O
synthetic	O
responses.	O

KBP	B-DatasetName
(Ling	O
and	O
Weld,	O
2012)	O
uses	O
Wikipedia	O
articles	O
annotated	O
with	O
Freebase	O
entries	O
as	O
the	O
training	O
set,	O
and	O
employs	O
manually-annotated	O
sentences	O
from	O
2013	O
KBP	B-TaskName
slot	I-TaskName
filling	I-TaskName
assessment	O
results	O
(Ellis	O
et	O
al,	O
2012)	O
as	O
the	O
extra	O
test	O
set.	O

In	O
this	O
paper	O
we	O
take	O
a	O
deeper	O
look	O
at	O
the	O
efficacy	O
of	O
strong	O
few-shot	O
classification	O
models	O
in	O
the	O
more	O
common	O
relation	B-TaskName
extraction	I-TaskName
setting,	O
and	O
show	O
that	O
typical	O
few-shot	O
evaluation	O
metrics	O
obscure	O
a	O
wide	O
variability	O
in	O
performance	O
across	O
relations.	O

EReader	B-MethodName
consistently	O
achieves	O
higher	O
performance	O
using	O
the	O
knowledge	O
retrieved	O
from	O
complete	O
corpus,	O
where	O
the	O
biggest	O
gain	O
of	O
7.86%	B-MetricValue
is	O
achieved	O
when	O
using	O
five	O
knowledge.	O

Key	B-TaskName
information	I-TaskName
extraction	I-TaskName
from	O
form-like	O
documents	O
is	O
one	O
of	O
the	O
fundamental	O
tasks	O
of	B-TaskName
document	I-TaskName
understanding	I-TaskName
that	O
has	O
many	O
real-world	O
applications.	O

We	O
initialize	O
our	O
context-aware	B-MethodName
ST	I-MethodName
with	O
the	O
sentence-level	O
Baseline,	O
i.e.	O
ST+AFS,	B-MethodName
and	O
then	O
finetune	O
the	O
model	O
for	O
20K	B-HyperparameterValue
steps	B-HyperparameterName
based	O
on	O
the	O
concatenation	O
method	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
around	O
40K	B-HyperparameterValue
subwords.	O

We	O
attribute	O
LCAMs	B-MethodName
superior	O
performance	O
to	O
(1)	O
Using	O
a	O
domain-specific	O
(biomedical)	O
language	O
representation	O
model	O
(BioBERT)	B-MethodName
at	O
its	O
encoding	O
layer,	O
(2)	O
Applying	O
label-specific	O
attention	O
prior	O
to	O
classifying	O
a	O
token	O
as	O
well	O
as	O
before	O
classifying	O
the	O
mean	O
pooled	O
representation	O
of	O
an	O
Example	O
Input	O
sentence	O
Predicted	O
labels	O
Predicted	O
labels	O
P@1	O
P@2	O

For	O
IEMOCAP,	B-DatasetName
our	O
model	O
obtained	O
a	O
weighted	O
average	O
F1	B-MetricName
score	O
of	O
65.22%,	B-MetricValue
outperforming	O
Di-alogueGCN	B-MethodName
by	O
more	O
than	O
1	O
point.	O

Given	O
that	O
BERT	B-MethodName
remains	O
one	O
of	O
the	O
most	O
effective	O
models	O
in	O
varieties	O
of	O
NLP	O
tasks,	O
and	O
especially	O
that	O
handcrafted	O
features	O
have	O
relatively	O
low	O
dimensionality	O
compared	O
to	O
BERT,	B-MethodName
we	O
believe	O
that	O
the	O
performance	O
gain	O
equivalent	O
to	O
20%	B-MetricValue
of	O
the	O
performance	O
gain	O
of	O
BERT	B-MethodName
is	O
substantial	O
for	O
this	O
completely	O
new	O
application	O
domain.	O

More	O
recently,	O
it	O
has	O
been	O
successfully	O
used	O
to	O
reduce	O
partial	O
input	O
biases	O
in	O
different	O
fields	O
of	O
NLP,	B-TaskName
such	O
as	B-TaskName
natural	I-TaskName
language	I-TaskName
inference	I-TaskName
(NLI)	B-TaskName
(Belinkov	O
et	O
al,	O
2019;Stacey	O
et	O
al,	O
2020)	O
and	B-TaskName
visual	I-TaskName
question	I-TaskName
answering	I-TaskName
(VQA)	B-TaskName
(Ramakrishnan	O
et	O
al,	O
2018).	O

Held-Out	O
Performance	O
Micro/macro	B-MetricName
F1	I-MetricName
scores	O
on	O
the	O
held-out	O
test	O
sets	O
corresponding	O
to	O
their	O
training	O
data	O
are	O
91.5/70.8	B-MetricValue
for	O
B-D	B-MethodName
and	O
92.9/70.3	B-MetricValue
for	O
B-F	B-MethodName
(Founta	O
et	O
al,	O
2018).	O

The	O
size	O
of	O
S2ORC	B-MethodName
makes	O
it	O
more	O
than	O
sufficient	O
for	O
pretraining	O
large	O
language	O
models	O
such	O
as	O
ELMO,	B-MethodName
BERT,	B-MethodName
ROBERTA,	B-MethodName
GPT2,	B-MethodName
and	O
others,	O
whose	O
reported	O
training	O
data	O
sizes	O
are	O
given	O
in	O
Table	O
9	O
for	O
comparison.	O

More	O
closely	O
related	O
to	O
our	O
task	O
is	O
the	O
context	B-TaskName
dependent	I-TaskName
semantic	I-TaskName
parsing	I-TaskName
task	O
on	O
the	B-DatasetName
ATIS	I-DatasetName
dataset	O
(Zettlemoyer	O
and	O
Collins,	O
2009;Suhr	O
et	O
al,	O
2018)	O
for	O
mapping	O
NL	O
to	O
database	O
queries	O
based	O
on	O
a	O
prior	O
history	O
of	O
NL	O
and	O
query	O
pairs.	O

We	O
further	O
trained	O
an	O
XLnet	B-MethodName
models	O
with	O
similar	O
training	O
hyperparameters	O
and	O
achieved	O
an	O
accuracy	B-MetricName
of	O
0.983.	B-MetricValue

Since	O
our	O
UmlsBERT	B-MethodName
model	O
is	O
focused	O
on	O
augmenting	O
the	O
Masked	O
LM	O
task	O
with	O
clinical	O
information	O
from	O
the	O
UMLS	B-DatasetName
Metathesaurus,	O
we	O
omit	O
the	O
description	O
of	O
the	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
task	O
and	O
only	O
describe	O
the	O
details	O
of	O
the	O
Masked	O
LM	O
task	O
herein.	O

We	O
use	O
the	O
publicly-provided	O
train	O
and	O
test	O
splits	O
for	O
the	O
sst2	B-DatasetName
and	O
20news	B-DatasetName
datasets	O
and	O
further	O
derive	O
a	O
validation	O
split	O
consisting	O
of	O
20%	B-HyperparameterValue
(v	B-HyperparameterName
=	O
0.2)	B-HyperparameterValue
of	O
the	O
train	O
split	O
(D	O
t	O
),	O
with	O
uniform	O
class	O
distribution.	O

Lastly,	O
to	O
better	O
understand	O
how	O
well	O
the	O
email	B-TaskName
thread	I-TaskName
summarization	O
models	O
perform	O
and	O
investigate	O
the	O
correlation	O
between	O
automatic	O
metrics	O
and	O
human	B-MethodName
judgment,	I-MethodName
we	O
ask	O
humans	O
to	O
rate	O
the	O
"salience"	O
(how	O
well	O
the	O
model	O
summarizes	O
salient	O
points)	O
and	O
"faithfulness"	O
(how	O
well	O
the	O
model	O
stays	O
true	O
to	O
the	O
email	O
thread)	O
of	O
model-generated	O
summaries,	O
as	O
well	O
as	O
to	O
perform	O
a	O
pairwise	O
comparison	O
between	O
our	O
best	O
and	O
base	O
models.	O

Baselines.	O
(1)	O
Ebner's	B-MethodName
(Ebner	O
et	O
al,	O
2020)	O
is	O
a	O
semantic	B-TaskName
role	I-TaskName
labeling-based	I-TaskName
method	O
with	O
greedy	O
decoding.	O
(2)	O
Zhang's	B-MethodName
(Zhang	O
et	O
al,	O
2020b)	O
is	O
a	O
two-step	O
head-based	O
model	O
that	O
first	O
predicts	O
headwords	O
of	O
an	O
argument	O
and	O
then	O
expands	O
to	O
the	O
full	O
span.	O

As	O
seen,	O
the	O
BLEU	B-MetricName
score	I-MetricName
of	O
best	O
"External	O
ZP	O
prediction"	O
model	O
dramatically	O
drops	O
by	O
-1.06	B-MetricValue
points,	O
showing	O
that	O
this	O
approach	O
is	O
heavily	O
dependent	O
on	O
the	O
results	O
of	O
external	O
ZP	O
annotations.	O

TARA	B-DatasetName
is	O
collected	O
via	O
a	O
rigorous	O
process	O
that	O
involves	O
rule-based	B-TaskName
distant	I-TaskName
supervision	I-TaskName
extraction	O
from	O
news-images	O
data	O
which	O
results	O
in	O
16k	O
image	O
examples.	O

For	O
instance,	O
Reformer	I-MethodName
(Kitaev	O
et	O
al,	O
2020)	O
is	O
tested	O
on	O
the	O
64k-chunk	B-DatasetName
en-wik8	B-DatasetName
dataset	O
for	O
unidirectional	O
language	O
modeling;	O
Performer	B-MethodName
(Choromanski	O
et	O
al,	O
2021)	O
reports	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
(MLM)	B-MetricName
perplexity	I-MetricName
on	O
the	O
PG-19	B-DatasetName
book	O
corpus	O
and	O
protein	O
sequences;	O
Linformer	B-MethodName
reports	O
MLP	B-MetricName
perplexity	I-MetricName
with	O
various	O
input	O
length,	O
while	O
most	O
of	O
the	O
documents	O
in	O
their	O
pretrain	O
corpus	O
are	O
short	O
documents.	O

Document	B-TaskName
summarization	I-TaskName
is	O
a	O
task	O
of	O
creating	O
a	O
concise	O
summary	O
from	O
a	O
given	O
document	O
while	O
keeping	O
the	O
original	O
content.	O

In	O
Case	O
3,	O
even	O
if	O
PinyinGPT-Concat	O
ranks	O
the	O
ground	O
truth	O
as	O
the	O
second	O
best,	O
the	O
top	O
1	O
prediction	O
still	O
makes	O
much	O
sense	O
and	O
fit	O
well	O
with	O
the	O
context.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
5e-5	B-HyperparameterValue
and	O
the	O
training	B-HyperparameterName
steps	I-HyperparameterName
are	O
5000	B-HyperparameterValue
for	O
both	O
base	O
and	O
large	O
models.	O

For	O
the	O
second	O
approach,	O
instead,	O
i.e.,	O
GOLDAMR-SILVERTRNS,	B-MethodName
we	O
choose	O
AMR	B-DatasetName
2.0	I-DatasetName
as	O
gold	O
dataset	O
and	O
translate	O
the	O
sentences	O
into	O
Chinese,	O
German,	O
Italian	O
and	O
Spanish.	O

On	O
OpenSQuAD	B-DatasetName
dataset,	O
our	O
DUREPA	B-MethodName
model	O
using	O
hybrid	O
evidences	O
achieves	O
a	O
new	O
state-ofthe-art	O
EM	B-MetricName
score	I-MetricName
of	O
57.0.	B-MetricValue

In	O
multilingual	O
settings,	O
we	O
tested	O
Babylon	B-MethodName
multilingual	I-MethodName
word	I-MethodName
embeddings	I-MethodName
and	O
MUSE	B-MethodName
(Lample	O
et	O
al,	O
2017)	O
on	O
the	O
different	O
tasks.	O

XQuAD	B-MethodName
(Artetxe	O
et	O
al,	O
2020)	O
is	O
a	O
human	O
translation	O
of	O
the	O
SQuAD	B-DatasetName
en	O
development	O
set	O
in	O
10	O
languages	O
(Arabic,	O
Chinese,	O
German,	O
Greek,	O
Hindi,	O
Russian,	O
Spanish,	O
Thai,	O
Turkish,	O
and	O
Vietnamese),	O
providing	O
1k	O
QA	O
pairs	O
for	O
each	O
language.	O

Figure	O
3	O
shows	O
the	O
confusion	O
matrices	O
of	O
PDTB	B-MethodName
3.0	I-MethodName
L2	O
classification	O
predictions,	O
again	O
from	O
XLNet-large	B-MethodName
and	O
BERT-large	B-MethodName
models	O
(we	O
did	O
not	O
observe	O
immediate	O
qualitative	O
differences	O
between	O
XLNet	B-MethodName
and	O
BERT,	B-MethodName
or	O
between	O
large	O
and	O
base	O
models).	O

That	O
is,	O
EPT-X	B-MethodName
generates	O
different	O
solution	O
equations	O
when	O
it	O
only	O
receives	O
the	O
generated	O
explanation	O
as	O
input	O
in	O
Phase	O
2.	O

To	O
investigate	O
whether	O
language	B-TaskName
generation	I-TaskName
models	O
can	O
serve	O
as	O
behavioral	O
priors	O
for	O
systems	O
deployed	O
in	O
social	O
settings,	O
we	O
evaluate	O
their	O
ability	O
to	O
generate	O
action	O
descriptions	O
that	O
achieve	O
predefined	O
goals	O
under	O
normative	O
constraints.	O

As	O
shown	O
in	O
Figure	O
3,	O
the	O
performance	O
gap	O
be-	O
tween	O
our	O
approach	O
and	O
the	O
baseline	O
goes	O
widest	O
for	O
AMR	B-DatasetName
graphs	O
with	O
more	O
than	O
5	O
reentrancies,	O
on	O
which	O
our	O
approach	O
outperforms	O
the	O
baseline	O
by	O
6.61	B-MetricValue
BLEU	B-MetricName
scores.	O

We	O
also	O
evaluate	O
the	O
transfer	O
performance	O
of	O
multilingual	B-TaskName
sentence	I-TaskName
embeddings	I-TaskName
on	O
downstream	O
classification	O
tasks	O
from	O
the	B-DatasetName
SentEval	I-DatasetName
benchmark	O
(Conneau	O
and	O
Kiela,	O
2018).	O

Regardless	O
of	O
the	O
initialization	O
method,	O
the	O
VECO	B-MethodName
initialized	O
model	O
can	O
gain	O
consistent	O
1∼2	B-MetricValue
BLEU	B-MetricName
improvement	O
over	O
the	O
randomly	O
initialized	O
model.	O

To	O
alleviate	O
the	O
issue,	O
we	O
train	O
a	O
binary	O
classifier	O
on	O
the	B-DatasetName
CoLA	I-DatasetName
corpus	O
(Warstadt	O
et	O
al,	O
2019)	O
to	O
learn	O
to	O
judge	O
the	O
grammaticality,	O
and	O
then	O
filter	O
out	O
those	O
examples	O
that	O
are	O
classified	O
as	O
ungrammatical	O
(the	O
classifier	B-MetricName
score	I-MetricName
less	B-MetricValue
than	O
0.5).	B-MetricValue

In	O
our	O
experiments,	O
we	O
keep	O
K	B-HyperparameterName
=	O
96	B-HyperparameterValue
(out	O
of	O
12	B-HyperparameterValue
×	I-HyperparameterValue
12	I-HyperparameterValue
=	O
144)	B-HyperparameterValue
heads	O
in	O
GPT-2	B-DatasetName
and	O
we	O
report	O
the	O
average	O
of	O
5	O
runs	O
with	O
different	O
random	O
seeds.	O

We	O
empirically	O
compare	O
a	O
wider	O
range	O
of	O
model	B-HyperparameterName
depths	I-HyperparameterName
for	O
Transformer+DS-Init+MAtt	B-MethodName
with	O
up	O
to	O
30	B-HyperparameterValue
layers.	B-HyperparameterName

Importantly,	O
we	O
create	O
pseudo-pretrained	O
embeddings	O
for	O
these	O
new	O
OA-NER-based	B-TaskName
tokens	O
by	O
adding	O
a	O
small	O
amount	O
of	O
random	O
Gaussian	O
noise	O
(mean	O
0	B-HyperparameterValue
and	O
variance	B-HyperparameterName
of	O
0.1)	B-HyperparameterValue
to	O
pre-trained	O
embeddings	O
(Pennington	O
et	O
al,	O
2014)	O
of	O
the	O
root	O
word	O
corresponding	O
to	O
the	O
category	O
(e.g.,	O
person).	O

We	O
use	O
the	O
Adam	B-MethodName
optimizer	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
with	O
warm-up.	O

Then,	O
we	O
apply	O
SummaReranker	B-MethodName
on	O
XSum	B-DatasetName
and	O
Reddit	B-DatasetName
TIFU,	I-DatasetName
as	O
shown	O
in	O
Table	O
6.	O

In	O
this	O
paper,	O
we	O
train	O
grammatical	B-MethodName
role	I-MethodName
probes	I-MethodName
on	O
the	O
embedding	O
spaces	O
of	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
1	O
,	O
and	O
evaluate	O
them	O
on	O
these	O
rare	O
non-prototypical	O
examples,	O
where	O
the	O
meaning	O
of	O
words	O
in	O
context	O
is	O
different	O
from	O
what	O
we	O
would	O
expect	O
from	O
looking	O
at	O
the	O
words	O
alone.	O

Paraphraser	O
We	O
finetune	O
the	O
paraphraser	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1,	B-HyperparameterValue
024	I-HyperparameterValue
tokens	O
for	O
5,	B-HyperparameterValue
000	I-HyperparameterValue
iterations	B-HyperparameterName
(500	B-HyperparameterValue
for	O
warm-up),	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
using	O
ADAM.	B-MethodName

Models	O
from	O
Related	O
Tasks	O
EM-DM	B-MethodName
achieves	O
very	O
high	O
precision	B-MetricName
(P	B-MetricName
,	O
P	O
w	O
)	O
for	O
the	O
minority	O
classes,	O
showing	O
a	O
clear	O
link	O
between	O
the	O
tasks	O
of	O
emotion	I-TaskName
recognition	I-TaskName
and	O
detecting	O
changes	O
in	O
a	O
user's	O
mood	O
-indeed,	O
emotionally	O
informed	O
mod-els	O
have	O
been	O
successfully	O
applied	O
to	O
post-level	O
classification	O
tasks	O
in	O
mental	O
health	O
(Sawhney	O
et	O
al,	O
2020a);	O
however,	O
both	O
EM	O
models	O
achieve	O
low	O
recall	B-MetricName
(R,	B-MetricName
R	O
w	O
)	O
for	O
IE/IS	O
compared	O
to	O
the	O
rest.	O

Experiments	O
on	O
the	O
publicly	O
available	O
data	O
sets,	O
i.e.,	O
WMT-5	B-DatasetName
and	O
OPUS-100,	B-DatasetName
show	O
that	O
the	O
proposed	O
method	O
achieves	O
substantial	O
improvements	O
over	O
strong	O
baselines.	O

We	O
found	O
that	O
the	O
annotators	O
disagree	O
substantially	O
more	O
as	O
compared	O
to	O
the	O
humanwritten	O
answers,	O
with	O
a	O
Fleiss	B-MetricName
kappa	I-MetricName
of	O
0.31	B-MetricValue
(vs.	O
0.45	B-MetricValue
for	O
human-written	B-MethodName
answers),	O
suggesting	O
that	O
the	O
discourse	O
structure	O
of	O
model-generated	O
answers	O
are	O
less	O
clear,	O
even	O
to	O
our	O
trained	O
annotators.	O

